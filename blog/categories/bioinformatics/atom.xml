<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: bioinformatics | bio(logist | informatician)]]></title>
  <link href="http://ctSkennerton.github.io/blog/categories/bioinformatics/atom.xml" rel="self"/>
  <link href="http://ctSkennerton.github.io/"/>
  <updated>2015-10-03T02:40:34-07:00</updated>
  <id>http://ctSkennerton.github.io/</id>
  <author>
    <name><![CDATA[Connor Skennerton]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Genome Bin Decontamination]]></title>
    <link href="http://ctSkennerton.github.io/blog/2015/10/03/Bin-Decontamination/"/>
    <updated>2015-10-03T00:00:00-07:00</updated>
    <id>http://ctSkennerton.github.io/blog/2015/10/03/Bin-Decontamination</id>
    <content type="html"><![CDATA[<p>Genome bins comming off automated pipelines can be contaminated
with parts of other genomes. As part of my workflow I use
<a href="http://genome.cshlp.org/content/early/2015/05/14/gr.186072.114.abstract">CheckM</a>
(I&rsquo;m biased since I&rsquo;m a coauthor) to assess the contamination of
genome bins using single-copy marker genes. If you&rsquo;re lucky then
the genome bins that you&rsquo;re interested in will be relatively complete
without much contamination. Unfortunately that isn&rsquo;t always the
case. In this blog post I&rsquo;m going to run through some of the analyses
that I did on a genome bin that was 90% complete but 70% contaminated.
This is exploratory analysis to see if I can manually improve the
bin over what the automated tools can do.</p>

<h2>Background</h2>

<p>I&rsquo;m not going into details on how I got this bin but briefly I had
three metagenomic samples, named 3730, 5133, and 5579, which were
assembled with
<a href="http://bioinformatics.oxfordjournals.org/content/31/10/1674.long">megahit</a>
and binned using <a href="https://peerj.com/articles/1165/">metabat</a>. I ran
the genome bins from sample 3730 through CheckM. In the below analysis I&rsquo;m going to
try to improve <strong>bin_41</strong>.</p>

<h2>Analysis</h2>

<h3>Creating the input files</h3>

<p>For starters I want the <code>.depth.txt</code> file created by metabat, which I copied and renamed <code>3730_coverage.tsv</code>. Next I created a mapping file for which contigs belonged to which bins. I have the fasta files of all of the bins in a separate directory. To get the mapping of the fasta files I ran the following:
<code>sh
grep -oP '(?&lt;=^\&gt;)\S+' *fa | awk 'BEGIN{FS=":";OFS="\t"}{print $2,$1}' &gt; 3730_bin_mapping.tsv
</code>
And finally I wanted to know where all of the multi-copy markers were so I created a file based on the CheckM output, with a bit of reformatting in awk:
<code>sh
checkm qa -o 6 ckm_results/lineage.ms ckm_results | awk 'BEGIN{FS="\t";OFS=","}{n = split($3,a,",");for(i = 1; i &lt;= n; ++i){print $1,$2,a[i]}}' &gt; 3730_multiple_markers.csv
</code></p>

<h3>Exploratory analysis in R</h3>

<p><div>
  <pre><code class='r'>library(readr)
duplicated_markers = read_csv(&amp;ldquo;3730_multiple_markers.csv&amp;rdquo;)
coverage = read_tsv(&amp;ldquo;3730_coverage.tsv&amp;rdquo;)&lt;/p&gt;

&lt;h1&gt;no header in the file, so give the columns names&lt;/h1&gt;

&lt;p&gt;bin_mapping = read_tsv(&amp;ldquo;3730_bin_mapping.tsv&amp;rdquo;, col_names = c(&amp;ldquo;contigName&amp;rdquo;, &amp;ldquo;bin&amp;rdquo;))</code></pre>
</div>

Clean up the dataframes to make the column names consistent and remove a bit of unneeded text. Note that these commands are specific to how files were named on my system, you may not need to do this or change this section to meet your own needs.</p>

<p><div>
  <pre><code class='r'>bin_mapping$bin = gsub(&amp;ldquo;metabat_binned/final.contigs.fa.metabat&lt;em&gt;&amp;rdquo;, &amp;ldquo;&amp;rdquo;, bin_mapping$bin)
bin_mapping$bin = gsub(&amp;ldquo;\.fa$&amp;rdquo;, &amp;ldquo;&amp;rdquo;, bin_mapping$bin)
duplicated_markers$&lt;code&gt;Bin Id&lt;/code&gt; = gsub(&amp;ldquo;final.contigs.fa.metabat&lt;/em&gt;&amp;rdquo;, &amp;ldquo;&amp;rdquo;, duplicated_markers$&lt;code&gt;Bin Id&lt;/code&gt;)
duplicated_markers$&lt;code&gt;Gene Ids&lt;/code&gt; = gsub(&amp;ldquo;_\d+$&amp;rdquo;, &amp;ldquo;&amp;rdquo;, duplicated_markers$&lt;code&gt;Gene Ids&lt;/code&gt;, perl=TRUE)
colnames(duplicated_markers) &amp;lt;&amp;ndash; c(&amp;ldquo;bin&amp;rdquo;, &amp;ldquo;marker&amp;rdquo;, &amp;ldquo;contigName&amp;rdquo;)</code></pre>
</div>

Create a smaller dataframe of just the binned contigs</p>

<p><div>
  <pre><code class='r'>binned_contigs = merge(bin_mapping, coverage, by=&amp;ldquo;contigName&amp;rdquo;)</code></pre>
</div>

We are interested in Bin 41, lets get just the data for it.</p>

<p><div>
  <pre><code class='r'>bin_41 = binned_contigs[binned_contigs$bin == &amp;ldquo;bins_41&amp;rdquo;,]</code></pre>
</div>

Since we have three samples we can look at the coverage of the contigs in each sample as a matrix of 2D</p>

<p><div>
  <pre><code class='r'>pairs(~final.contigs.3730.bam+final.contigs.5133.bam+final.contigs.5579.bam, data=bin_41, main = &amp;ldquo;Comparison of contig coverage between samples&amp;rdquo;, labels = c(&amp;ldquo;3730&amp;rdquo;, &amp;ldquo;5133&amp;rdquo;, &amp;ldquo;5579&amp;rdquo;))</code></pre>
</div>
</p>

<p><img src="/images/bin-decontamination-paired-coverage-1.svg" alt="plot of chunk bin-decontamination-paired-coverage" />
The majority of the contigs are found in sample 3730 between 10-15x and with very low coverage in sample 5133. Some of the contigs also have some coverage in sample 5579, but most don&rsquo;t. It&rsquo;s tempting to remove all of the contigs that don&rsquo;t fit into the band of coverage, but from this we can&rsquo;t be certain if the duplicated markers are in these outlier contigs.</p>

<p>Since the coverage of sample 5133 doesn&rsquo;t really factor into things lets look at a single 2D plot of the 3730 coverage and the 5579 coverage. As a coarse approach lets take a look at where all of the contigs with duplicated markers are in this plot. For starters make a new column in the bin_41 dataframe that tells us if the contig contains any duplicated marker.</p>

<p><div>
  <pre><code class='r'>bin_41_duplicated_markers = subset(duplicated_markers, bin == &amp;ldquo;bins_41&amp;rdquo;)
bin_41$containsDuplicateMarker &amp;lt;&amp;ndash; bin_41$contigName %in% unique(bin_41_duplicated_markers$contigName)</code></pre>
</div>

And now lets plot the data, changing the color of the points based on the value of the &lsquo;containsDuplicateMarker&rsquo; column</p>

<p><div>
  <pre><code class='r'>plot(bin_41$final.contigs.3730.bam, bin_41$final.contigs.5579.bam, pch = 19, col = &amp;ldquo;lightgrey&amp;rdquo;, cex=0.5, xlab=&amp;ldquo;3730 coverage&amp;rdquo;, ylab=&amp;ldquo;5579 coverage&amp;rdquo;, main=&amp;ldquo;Contigs containing duplicated markers&amp;rdquo;)
points(bin_41[bin_41$containsDuplicateMarker,]$final.contigs.3730.bam, bin_41[bin_41$containsDuplicateMarker,]$final.contigs.5579.bam, col = &amp;ldquo;red&amp;rdquo;)</code></pre>
</div>
</p>

<p><img src="/images/bin-decontamination-all-duplicate-markers-1.svg" alt="plot of chunk bin-decontamination-all-duplicate-markers" />
Apart from a couple of outliers, the majority of the contigs that contain multiple markers are in the central mass of contigs. There doesn&rsquo;t appear to be any way to systematically remove a substantial amount of contamination in this genome bin.</p>

<p>Instead of looking at all contigs that contain any duplicated markers, we can also visualise the positions of contigs for a specific duplicated marker. You can get a list of the markers that are duplicated in the bin by using the unique command. Then create a logical vector of bin_41 of the contigs that contain any of the particular markers.</p>

<p><div>
  <pre><code class='r'>unique(bin_41_duplicated_markers$marker)</code></pre>
</div>
</p>

<p><div>
  <pre><code class='text'>&lt;/p&gt;

&lt;h2&gt;[1] &amp;ldquo;PF03439.8&amp;rdquo;  &amp;ldquo;PF01172.13&amp;rdquo; &amp;ldquo;PF00164.20&amp;rdquo; &amp;ldquo;PF09173.6&amp;rdquo;  &amp;ldquo;PF03874.11&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[6] &amp;ldquo;PF00203.16&amp;rdquo; &amp;ldquo;PF00466.15&amp;rdquo; &amp;ldquo;PF00416.17&amp;rdquo; &amp;ldquo;PF05670.8&amp;rdquo;  &amp;ldquo;PF04127.10&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[11] &amp;ldquo;PF01090.14&amp;rdquo; &amp;ldquo;PF01193.19&amp;rdquo; &amp;ldquo;PF09249.6&amp;rdquo;  &amp;ldquo;PF11987.3&amp;rdquo;  &amp;ldquo;PF00298.14&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[16] &amp;ldquo;PF00252.13&amp;rdquo; &amp;ldquo;PF01198.14&amp;rdquo; &amp;ldquo;PF01201.17&amp;rdquo; &amp;ldquo;PF01351.13&amp;rdquo; &amp;ldquo;PF04567.12&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[21] &amp;ldquo;PF01922.12&amp;rdquo; &amp;ldquo;PF01667.12&amp;rdquo; &amp;ldquo;PF00398.15&amp;rdquo; &amp;ldquo;TIGR03685&amp;rdquo;  &amp;ldquo;PF04983.13&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[26] &amp;ldquo;TIGR03683&amp;rdquo;  &amp;ldquo;PF04561.9&amp;rdquo;  &amp;ldquo;PF04565.11&amp;rdquo; &amp;ldquo;PF04919.7&amp;rdquo;  &amp;ldquo;TIGR00425&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[31] &amp;ldquo;PF01200.13&amp;rdquo; &amp;ldquo;PF07541.7&amp;rdquo;  &amp;ldquo;PF00679.19&amp;rdquo; &amp;ldquo;PF01194.12&amp;rdquo; &amp;ldquo;PF13656.1&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[36] &amp;ldquo;PF01015.13&amp;rdquo; &amp;ldquo;PF01282.14&amp;rdquo; &amp;ldquo;PF01287.15&amp;rdquo; &amp;ldquo;PF00276.15&amp;rdquo; &amp;ldquo;PF01780.14&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[41] &amp;ldquo;TIGR00064&amp;rdquo;  &amp;ldquo;TIGR00389&amp;rdquo;  &amp;ldquo;PF04997.7&amp;rdquo;  &amp;ldquo;PF04563.10&amp;rdquo; &amp;ldquo;PF03947.13&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[46] &amp;ldquo;PF01912.13&amp;rdquo; &amp;ldquo;PF00181.18&amp;rdquo; &amp;ldquo;PF04010.8&amp;rdquo;  &amp;ldquo;TIGR00549&amp;rdquo;  &amp;ldquo;TIGR01018&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[51] &amp;ldquo;PF01157.13&amp;rdquo; &amp;ldquo;TIGR00392&amp;rdquo;  &amp;ldquo;PF00347.18&amp;rdquo; &amp;ldquo;PF00831.18&amp;rdquo; &amp;ldquo;TIGR01080&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[56] &amp;ldquo;TIGR00408&amp;rdquo;  &amp;ldquo;TIGR00270&amp;rdquo;  &amp;ldquo;PF08068.7&amp;rdquo;  &amp;ldquo;PF00687.16&amp;rdquo; &amp;ldquo;PF04560.15&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[61] &amp;ldquo;PF05000.12&amp;rdquo; &amp;ldquo;PF00380.14&amp;rdquo; &amp;ldquo;PF00752.12&amp;rdquo; &amp;ldquo;PF09377.5&amp;rdquo;  &amp;ldquo;TIGR02389&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[66] &amp;ldquo;TIGR00419&amp;rdquo;  &amp;ldquo;PF01000.21&amp;rdquo; &amp;ldquo;PF04566.8&amp;rdquo;  &amp;ldquo;PF00562.23&amp;rdquo; &amp;ldquo;PF00366.15&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[71] &amp;ldquo;TIGR00057&amp;rdquo;  &amp;ldquo;PF01191.14&amp;rdquo; &amp;ldquo;PF00572.13&amp;rdquo; &amp;ldquo;PF01981.11&amp;rdquo; &amp;ldquo;PF01984.15&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[76] &amp;ldquo;TIGR03677&amp;rdquo;  &amp;ldquo;PF00411.14&amp;rdquo; &amp;ldquo;PF01864.12&amp;rdquo; &amp;ldquo;PF00238.14&amp;rdquo; &amp;ldquo;TIGR03724&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[81] &amp;ldquo;PF00177.16&amp;rdquo; &amp;ldquo;PF00827.12&amp;rdquo; &amp;ldquo;TIGR02338&amp;rdquo;  &amp;ldquo;PF01849.13&amp;rdquo; &amp;ldquo;TIGR00468&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[86] &amp;ldquo;PF01866.12&amp;rdquo; &amp;ldquo;PF01982.11&amp;rdquo; &amp;ldquo;PF00237.14&amp;rdquo; &amp;ldquo;PF03876.12&amp;rdquo; &amp;ldquo;PF02978.14&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[91] &amp;ldquo;TIGR00670&amp;rdquo;  &amp;ldquo;PF00623.15&amp;rdquo; &amp;ldquo;PF01246.15&amp;rdquo; &amp;ldquo;PF00573.17&amp;rdquo; &amp;ldquo;PF00189.15&amp;rdquo;&lt;/h2&gt;

&lt;h2&gt;[96] &amp;ldquo;TIGR01213&amp;rdquo;  &amp;ldquo;TIGR00344&amp;rdquo;  &amp;ldquo;PF01868.11&amp;rdquo; &amp;ldquo;PF03764.13&amp;rdquo; &amp;ldquo;PF00867.13&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;</code></pre>
</div>
</p>

<p><div>
  <pre><code class='r'>contains_x_marker = bin_41$contigName %in% subset(bin_41_duplicated_markers, marker == &amp;ldquo;PF01157.13&amp;rdquo;)$contigName</code></pre>
</div>

With this vector we can plot the points like we did with the complete set of duplicated markers.</p>

<p><div>
  <pre><code class='r'>plot(bin_41$final.contigs.3730.bam, bin_41$final.contigs.5579.bam, pch = 19, col = &amp;ldquo;lightgrey&amp;rdquo;, cex=0.5, main=&amp;ldquo;Position of contigs with PF01157.13&amp;rdquo;, xlab=&amp;ldquo;3730 coverage&amp;rdquo;, ylab=&amp;ldquo;5579 coverage&amp;rdquo;)
points(bin_41[contains_x_marker,]$final.contigs.3730.bam, bin_41[contains_x_marker,]$final.contigs.5579.bam, col = &amp;ldquo;red&amp;rdquo;)</code></pre>
</div>
</p>

<p><img src="/images/bin-decontamination-single-marker-positions-1.svg" alt="plot of chunk bin-decontamination-single-marker-positions" /></p>

<p>So now we can see that ther are three contigs containing this marker, one
appears to be an outlier, but the other two contigs have quite similar
coverage profiles</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The pace of genome binning from metagenomes]]></title>
    <link href="http://ctSkennerton.github.io/blog/2015/06/17/The-pace-of-genome-binning-from-metagenomics/"/>
    <updated>2015-06-17T00:00:00-07:00</updated>
    <id>http://ctSkennerton.github.io/blog/2015/06/17/The-pace-of-genome-binning-from-metagenomics</id>
    <content type="html"><![CDATA[<p>With the pace of science what seemed top stuff three years ago is now an
order of magnitude less than what just got published.</p>

<!--more-->


<p>When I was in my PhD I was working on
<a href="https://en.wikipedia.org/wiki/Enhanced_biological_phosphorus_removal">EBPR</a>
communities using metagenomics to characterize the the microbial and
phage populations. From all of my microbial data I could get about 70 draft
bacterial genomes. At the time (2012) that was pretty huge  considering the first
big paper on genome binning from metagenomes got 49 draft bacteria (<a href="http://www.sciencemag.org/content/337/6102/1661">Wrighton et al, 2012</a>).
The next year a similar study from EBPR recovered 31 draft genome bins
(<a href="http://www.nature.com/nbt/journal/v31/n6/abs/nbt.2579.html">Albertsen et al,
2013</a>). One of the big results
from all this computational effort is that a number of genomes from
<a href="https://en.wikipedia.org/wiki/Candidate_division">candidate phyla</a>. In
2012 I was super excited that I had multiple genomes from
<a href="https://en.wikipedia.org/wiki/Candidate_division_TM7">Saccharibacteria</a>
and
<a href="http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=67820">Microgenomates</a>
which at the time had only a few genome representatives. At the time I
thought I had the richest dataset in the world (which I most definitely
did not) now three years later and ~800 genome bins just got published
mostly from those candidate phyla I just mentioned (<a href="http://www.nature.com/nature/journal/vaop/ncurrent/full/nature14486.html">Brown et al, 2015</a>).
It seems like the magnitude of the cutting edge just went up a notch&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Genome Bin Completeness and Contamination]]></title>
    <link href="http://ctSkennerton.github.io/blog/2015/01/29/Genome-bin-completeness-and-contamination/"/>
    <updated>2015-01-29T00:00:00-08:00</updated>
    <id>http://ctSkennerton.github.io/blog/2015/01/29/Genome-bin-completeness-and-contamination</id>
    <content type="html"><![CDATA[<p>The question that most people ask when looking at a metagenomic draft genome bin
is: should this gene really be there? The answer is that sometimes it&rsquo;s not easy to know</p>

<!--more-->


<p>I spend a lot of my time looking at genome assemblies. They are almost always
from metagenomic data and usually are from some novel phylogenetic lineage
with few (if any) &ldquo;close&rdquo; relatives. Unfortunately the quality of genome
assemblies is such that all of these genomes are in many pieces and due to the
nature of binning, there is always the spectre of some genes not belonging to
the genome that your analyzing. I&rsquo;m currently a middle author on a
<a href="https://peerj.com/preprints/554/">paper</a> that attempts to quantify genome
completeness and contamination using single copy marker genes. The idea is that
there are genes that are always found in a single copy per genome, therefore
if you find them in multiple copies then you&rsquo;ve got some contamination from
other sources. This is great in principle but the number of single-copy genes
is going to be very small for a large set of genomes, for example there are
about 100 of these genes that are common for all bacteria. Assuming that some
bacterial genome has approximately 3000 genes, then looking at only 100 of them
is about 3% of the gene content. That&rsquo;s quite a small percentage to be making
big statements on how good or bad a genome is.</p>

<p>So here is a question that one of my colleagues posed in a group meeting: if a genome has 10%
contamination does that mean that 1 in 10 genes shouldn&rsquo;t be there?</p>

<p>My answer is: of course not, it means that 1 in 10 of the marker genes are in
multiple copies. But it does get me thinking, how can we tell that 10% of all
genes in the genome aren&rsquo;t from contamination?</p>

<p>One of the observations with
single-copy marker genes is that many of them are
co-located such that getting one erroneous contig can disproportionatly increase
contamination. Similarly all of the genes of a contig are either correctly or
incorrectly binned, rather than every 10th gene in some contig being from a
different genome. If we were to assume a perfect situation where every contaminating
contig contained at least one marker gene then the real value of contamination would be
the total number of genes on those contaminating contigs divided by the total number
of genes in the genome bin. The &ldquo;real&rdquo; contamination percent then becomes more
about the size of the contigs rather than the number of marker genes. This is
easy in theory but hard to determine in practice since you need to know which
copy of the marker gene is the contaminant and which is legit.</p>

<p>It&rsquo;s easy to remove
contigs with extra marker genes to lower the contamination numbers but what about
short contigs that contain no markers? I don&rsquo;t think there is a perfect way
to tell if they belong to a genome or not but one approach has been to look for
paired reads that link these contigs to larger ones in the assembly even if they
are not contiguous. Here again there are questions raised, if there are reads that
link the contig to the bigger assembly why is it separate, why hasn&rsquo;t the assembler
joined the contigs together? The magic behind genome assemblers creates many
interesting outcomes that I can never explain and as such there is no general rule
for believing paired-end links or not. Honestly, I usually go with my gut
(very unscientific) and generally only bother to go further in depth if there are
genes that are important to the story I&rsquo;m trying to write.</p>

<p>The golden rule of interpreting genome bin assemblies is that
the bigger the contig the more confident that you become in assigning
them to a draft genome.
(Of course the answer is just getting a better assembly :P).</p>

<p>With these observations and caveats in mind, what value is there in placing a
percent completeness or contamination based on marker genes if it doesn&rsquo;t really
relate to the total gene content?</p>

<p>I think there is a lot of value but not when the numbers are taken literally.
Marker genes are an approximation and
a way of sorting genomes that are worth a closer look. From experience I can
tell you that a genome that is
&ldquo;90% complete, 5% contamination&rdquo; looks a lot better than one that is
&ldquo;50% complete, 30% contamination&rdquo;. I take these numbers to be a relative score
along with other factors like the total number of contigs (less is better) and the
total number of bases (does it look like it&rsquo;s about the size of a whole genome)
to point me in the right direction of what I should work on first.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Uploading your Theis Literature Review to Wikipedia]]></title>
    <link href="http://ctSkennerton.github.io/blog/2014/11/12/Uploading-your-Theis-Lit-Review-to-Wikipedia/"/>
    <updated>2014-11-12T00:00:00-08:00</updated>
    <id>http://ctSkennerton.github.io/blog/2014/11/12/Uploading-your-Theis-Lit-Review-to-Wikipedia</id>
    <content type="html"><![CDATA[<p>Every PhD student can contribute to open science by uploading their thesis
literature review onto Wikipedia!</p>

<!-- more -->


<p>My thesis was entitled &ldquo;Phage-host evolution in a model ecosystem&rdquo;, where I
tracked the evolution of phage genome evolution and the evolution of bacterial
defense mechanisms using metagenomics. When I was writing my thesis I spent a
lot of time writing up the section on
<a href="http://en.wikipedia.org/wiki/CRISPR">CRISPRs</a>, which are a type of bacterial
adaptive immune system. The CRISPR field has exploded in recent times due to its
applications for genome editing which has also meant that there have been
numerous great reviews on the ever expanding primary literature. Despite this
though I found the CRISPR page on wikipedia to be a bit bare (of course now that
I&rsquo;ve updated it things look better, but check out the edit history for a look at
what I&rsquo;ve added). Since there was already a saturation of reviews on the subject
I decided to go through and add in what I had written for my thesis to
wikipedia. By far the most important <strong>and difficult</strong> part of this is was
formatting the references right. So I made a simple workflow for getting what I
wanted out of my thesis and into wikipedia.</p>

<ol>
<li>First thing, I wrote my thesis in word and used endnote to format my
references using the style from nucleic acids research (which is a numbered
style). If you want to use my method you need to have the exact formatting
style</li>
<li>Export the literature review from word as plain text and then removed
all the parts that aren&rsquo;t needed</li>
<li>Put back in some formatting for the headers using
<a href="https://en.wikipedia.org/wiki/Help:Cheatsheet">wikipedia&rsquo;s markup</a></li>
<li>Split out the bibliography into a separate file</li>
<li>Then use some very dodgy <a href="http://github.com/ctSkennerton/wikipedia_reference_formatter">perl code</a>
I wrote to format all of the in text citations into the format that wikipedia
requires</li>
</ol>


<p>This perl code takes the text and looks for numbers, or lists of numbers inside
parentheses. For example: (1,2,5), (1-7,14), (1). Then looks up what those
references are in the bibliography, extracting the title of the journal article.
Then it looks up what the pubmed ID is for that paper using
<a href="http://www.ncbi.nlm.nih.gov/books/NBK179288/">entrezdirect</a> with the title as
the search term. After getting the pubmed IDs for all the cited references it
saves them to a file, so that it can be used later on (without looking up the
pubmed IDs again) and any errors can be fixed manually. Finally all of those in
text citations are replaced with the proper wikipedia markup and your ready to
upload.</p>

<p>There are so many scientific wikipedia pages that are way too brief for their
subject matter and I&rsquo;m not just talking about the thousands of stubs for
bacterial and archaeal species either. My current work is with methane seep
sediment communities that perform <a href="https://en.wikipedia.org/wiki/Anaerobic_oxidation_of_methane">anaerobic oxidation of
methane</a> (AOM),
the wikipedia page contains five paragraphs. Looking at the broader topic of
<a href="https://en.wikipedia.org/wiki/Methanogenesis">methanogenesis</a> there is still
only three short paragraphs on the biochemistry. We know so much more than this,
all that knowledge fills the pages of hundreds of review articles yet when
someone, anyone types anything into google the first match that comes up is
wikipedia. We should make it better cause it&rsquo;s ultimately the most visible
publication on the internet.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My First Useful Script]]></title>
    <link href="http://ctSkennerton.github.io/blog/2014/06/23/My-first-useful-script/"/>
    <updated>2014-06-23T02:06:00-07:00</updated>
    <id>http://ctSkennerton.github.io/blog/2014/06/23/My-first-useful-script</id>
    <content type="html"><![CDATA[<p>Can you remember the first useful thing that you ever coded? I sure can, and I&rsquo;m
thankful for it every day.</p>

<!-- more -->


<p>I&rsquo;ve recently finished writing a little program called <a href="https://github.com/ctSkennerton/fxtract"><code>fxtract</code></a>
(which I&rsquo;ve blogged about <a href="/blog/2013/10/28/testing-out-seqans-multipattern-search-implementations/">before</a>)
that acts like grep but returns whole fasta or fastq records from a file. It&rsquo;s
taken me a very long time to write this thing, primarily cause I&rsquo;m writing it
in C++ but now that it&rsquo;s pretty much done I&rsquo;m feeling nostalgic about why I&rsquo;m
writing it in the first place.</p>

<p>Back in 2010 I was just starting my PhD with <a href="http://www.ecogenomic.org/users/gene-tyson">Gene Tyson</a>
and learning some bioinformatics on a &ldquo;toy&rdquo; dataset (which became my first
<a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0020095">paper</a>)
that was a huge 10 Mbp of viral sequence data! At the time I was learning Perl
using this great <a href="http://www.woolfit.net/perl/classindex.html">online tutorial</a>
but I found that I needed some more &ldquo;real-world&rdquo; examples to really get to grips
with the language. I can&rsquo;t fully remember what I was doing but I had a tabular
blast output file and I wanted to get the sequence of some contigs with blast
hits. I jumped at the opportunity to solve this with my nasent coding skills and
so I started writing my first useful bit of code:
<a href="https://github.com/ctSkennerton/scriptShed/blob/master/contig_extractor.pl"><code>contig_extractor.pl</code></a>.</p>

<p>In the beginning it did only what I first wrote it to do: take a blast file and
a fasta file and return some contigs. Over time though it morphed into something
more general &ndash; a way of getting some subset of a fasta file using a list of
identifiers. Over the years I added functionality for different file formats and
allowed searching using regular expressions.
This one piece of code eventually disseminated throughout the whole lab group
as new people came in and needed to solve the same problems. I think that the
introduction to github for many of the PhD students who came after me
was downloading my random collection of scripts just so they
could get their hands on <code>contig_extractor.pl</code>.</p>

<p>Of course every bioinformatician has probably written the same piece of code
to solve the same problem. If I had been smart enough to look I&rsquo;m sure
that I would have found one, but using a script doesn&rsquo;t quite give the same
level of satisfaction as writing it yourself (and finding that it&rsquo;s useful by all your
colleages).</p>

<p>There came a time though when <code>contig_extractor.pl</code> reached it&rsquo;s limit. I was
trying to extract a few thousand records from a fastq file with 100 million and
Perl just wasn&rsquo;t cutting it anymore. And so <code>fxtract</code> was born to do all the good
things from <code>contig_extractor.pl</code> just a heap faster.The sun is setting on my
first useful piece of code, at the moment only my finger memory and force of habit
keeps it being used. But when I think about it, it&rsquo;s been an amazing four year run for something that I
probably thought was going to be a one-off script, hopefully <code>fxtract</code> can get
as much love from me as well.</p>
]]></content>
  </entry>
  
</feed>
